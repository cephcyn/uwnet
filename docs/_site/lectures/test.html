<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Test Space</title>
<style>
foreignObject {overflow:visible}
</style>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <!--script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script-->

    <script src="https://www.desmos.com/api/v1.5/calculator.js?apiKey=dcb31709b452b1cf9dc26972add0fda6"></script>

    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:false,
        flowchart:{
            curve:'basis',
            htmlLabels:true,
        },
        theme: 'default',
        themeVariables:{
            edgeLabelBackground:'#fff',
        },
    });
    window.onload = function(){
                mermaid.init();

                window.MathJax = {
                  tex: {
                    macros: {
                      d: "{\\nabla}",
                    }
                  }
                };

                (function () {
                  var script = document.createElement('script');
                  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
                  script.async = true;
                  document.head.appendChild(script);
                })();
        
    };
    </script>
  </head>
  <body>
    <h2 id="neural-network-example">Neural Network Example</h2>
<figure class="chart">
<div class="mermaid">

graph LR
    x1(("\(x_1\)")) --&gt;|"\(w_1\)"| y1(("\(y_1\)"))
    x2(("\(x_2\)")) --&gt;|"\(w_2\)"| y1
    x3(("\(x_3\)")) --&gt;|"\(w_3\)"| y1
    x4(("\(x_4\)")) --&gt;|"\(w_4\)"| y1
    x1 --&gt; |"\(w_5\)"| y2(("\(y_1\)"))
    x2 --&gt; |"\(w_6\)"| y2
    x3 --&gt; |"\(w_7\)"| y2
    x4 --&gt; |"\(w_8\)"| y2
    y1 --&gt; |"\(v_1\)"| z(("\(z\)"))
    y2 --&gt; |"\(v_2\)"| z

</div>
<figcaption>An example, 2 layer neural network. The first layer is computed as \(y = wx\). The second layer is computed as \(z = vy\)</figcaption>
</figure>

<h2 id="loss-functions">Loss Functions</h2>
<figure class="chart">
<div class="mermaid">

graph LR
    a["model parameters <br /> (weights)"] --&gt; model
    subgraph Loss Function
    model --&gt; | predictions | error
    data --&gt; | labels | error
    data --&gt; | features | model
    end
    error --&gt; loss

</div>
<figcaption>The loss function takes as input the parameters to the model. It uses those parameters to calculate predictions based on features from the data. It compares these predictions to the labels from that data and outputs the calculated loss.</figcaption>
</figure>

<h2 id="forward-propagation-through-connected-layer">Forward Propagation Through Connected Layer</h2>
<figure class="chart">
<div class="mermaid">

graph LR
    x["\(x\)"] --&gt; wx["\(wx\)"]
    subgraph Connected Layer
    w["\(w\)"] --&gt; wx
    wx --&gt; wxb["\(wx+b\)"]
    b["\(b\)"] --&gt; wxb
    end
    wxb --&gt; y["\(y\)"]


</div>
<figcaption>Forward propagation through a connected layer. The input \(x\) is multiplied by the weights \(w\) and added to the biases \(b\). The output is \(y = wx + b\)</figcaption>
</figure>

<h2 id="backward-propagation-through-connected-layer">Backward Propagation Through Connected Layer</h2>
<figure class="chart">
<div class="mermaid">

graph RL
    dLdy["\(\frac{d L}<br />{d y}\)"] --&gt; dLdwxb["\(\frac{d L}<br />{d wx + b}\)"]
    subgraph Connected Layer
    dLdwxb --&gt; |aggregate| dLdb["\(\frac{d L}<br />{d b}\)"]
    dLdwxb --&gt; dLdwx["\(\frac{d L}<br />{d wx}\)"]
    dLdwx --&gt; dLdx["\(\frac{d L}<br />{d x}\)"]
    dLdwx --&gt; dLdw["\(\frac{d L}<br />{d w}\)"]
    w["\(w\)"] --&gt; |"\(\frac{d wx}<br />{d x}\)"| dLdx
    end
    x["\(x\)"] --&gt; |"\(\frac{d wx}<br />{d w}\)"|dLdw
    dLdx --&gt; dLdxout["\(\frac{d L}<br />{d x}\)"]

</div>
<figcaption>Backward propagation through a connected layer. The input is the \(\d_x\)</figcaption>
</figure>

<h2 id="backward-propagation-through-connected-layer-1">Backward Propagation Through Connected Layer</h2>
<figure class="chart">
<div class="mermaid">

graph RL
    dLdy["\(\d_y L\)"] --&gt; dLdwxb["\(\d_{wx + b}L\)"]
    subgraph Connected Layer
    dLdwxb --&gt; |aggregate| dLdb["\(\d_{b}L\)"]
    dLdwxb --&gt; dLdwx["\(\d_{wx}L\)"]
    dLdwx --&gt; dLdx["\(\d_{x}L\)"]
    dLdwx --&gt; dLdw["\(\d_{w}L\)"]
    w["\(w\)"] --&gt; |"\(\d_{x}wx = w^T\)"| dLdx
    end
    x["\(x\)"] --&gt; |"\(\d_{w}wx = x^T\)"|dLdw
    dLdx --&gt; dLdxout["\(\d_{x}L\)"]

</div>
<figcaption>Backward propagation through a connected layer. The input is the gradient of the loss with respect to \(y\): \(\d_y L\) which is equivalent to the gradient of the loss with respect to the weighted sum: \(\d_{wx+b} L\). By aggregating over examples in the batch we get the gradient with respect to the bias: \(\d_bL\). We also know the equality \(\d_{wx + b}L = \d_{wx}L\).</figcaption>
</figure>


  </body>
</html>
